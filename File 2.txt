Recall the major steps in inverted index construction:
1. Collect the documents to be indexed.
2. Tokenize the text.
3. Do linguistic preprocessing of tokens.
4. Index the documents that each term occurs in.
In this chapter we first briefly mention how the basic unit of a document can
be defined and how the character sequence that it comprises is determined

(Section 2.1). We then examine in detail some of the substantive linguis-
tic issues of tokenization and linguistic preprocessing, which determine the

vocabulary of terms which a system uses (Section 2.2). Tokenization is the

process of chopping character streams into tokens, while linguistic prepro-
cessing then deals with building equivalence classes of tokens which are the

set of terms that are indexed. Indexing itself is covered in Chapters 1 and 4.
Then we return to the implementation of postings lists. In Section 2.3, we

examine an extended postings list data structure that supports faster query-
ing, while Section 2.4 covers building postings data structures suitable for

handling phrase and proximity queries, of the sort that commonly appear in
both extended Boolean models and on the web.
2.1 Document delineation and character sequence decoding
2.1.1 Obtaining the character sequence in a document
Digital documents that are the input to an indexing process are typically
bytes in a file or on a web server. The first step of processing is to convert this

byte sequence into a linear sequence of characters. For the case of plain En-
glish text in ASCII encoding, this is trivial. But often things get much more

Online edition (c)

2009 Cambridge UP

20 2 The term vocabulary and postings lists

complex. The sequence of characters may be encoded by one of various sin-
gle byte or multibyte encoding schemes, such as Unicode UTF-8, or various

national or vendor-specific standards. We need to determine the correct en-
coding. This can be regarded as a machine learning classification problem,

as discussed in Chapter 13,

1 but is often handled by heuristic methods, user
selection, or by using provided document metadata. Once the encoding is
determined, we decode the byte sequence to a character sequence. We might

save the choice of encoding because it gives some evidence about what lan-
guage the document is written in.

The characters may have to be decoded out of some binary representation
like Microsoft Word DOC files and/or a compressed format such as zip files.
Again, we must determine the document format, and then an appropriate
decoder has to be used. Even for plain text documents, additional decoding

may need to be done. In XML documents (Section 10.1, page 197), charac-
ter entities, such as &amp;, need to be decoded to give the correct character,

namely & for &amp;. Finally, the textual part of the document may need to
be extracted out of other material that will not be processed. This might be
the desired handling for XML files, if the markup is going to be ignored; we
would almost certainly want to do this with postscript or PDF files. We will
not deal further with these issues in this book, and will assume henceforth
that our documents are a list of characters. Commercial products usually
need to support a broad range of document types and encodings, since users

want things to just work with their data as is. Often, they just think of docu-
ments as text inside applications and are not even aware of how it is encoded

on disk. This problem is usually solved by licensing a software library that
handles decoding document formats and character encodings.

The idea that text is a linear sequence of characters is also called into ques-
tion by some writing systems, such as Arabic, where text takes on some

two dimensional and mixed order characteristics, as shown in Figures 2.1
and 2.2. But, despite some complicated writing system conventions, there

is an underlying sequence of sounds being represented and hence an essen-
tially linear structure remains, and this is what is represented in the digital

representation of Arabic, as shown in Figure 2.1.
2.1.2 Choosing a document unit
DOCUMENT UNIT The next phase is to determine what the document unit for indexing is. Thus

far we have assumed that documents are fixed units for the purposes of in-
dexing. For example, we take each file in a folder as a document. But there

1. A classifier is a function that takes objects of some sort and assigns them to one of a number
of distinct classes (see Chapter 13). Usually classification is done by machine learning methods
such as probabilistic models, but it can also be done by hand-written rules.