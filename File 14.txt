The document representation in Naive Bayes is a sequence of terms or a bi-
nary vector he1

, . . . ,e|V|
i ∈ {0, 1}
|V|. In this chapter we adopt a different
representation for text classification, the vector space model, developed in

Chapter 6. It represents each document as a vector with one real-valued com-
ponent, usually a tf-idf weight, for each term. Thus, the document space X,

the domain of the classification function γ, is R|V|

. This chapter introduces a
number of classification methods that operate on real-valued vectors.
The basic hypothesis in using the vector space model for classification is

CONTIGUITY the contiguity hypothesis.
HYPOTHESIS

Contiguity hypothesis. Documents in the same class form a contigu-
ous region and regions of different classes do not overlap.

There are many classification tasks, in particular the type of text classification
that we encountered in Chapter 13, where classes can be distinguished by
word patterns. For example, documents in the class China tend to have high
values on dimensions like Chinese, Beijing, and Mao whereas documents in the
class UK tend to have high values for London, British and Queen. Documents
of the two classes therefore form distinct contiguous regions as shown in
Figure 14.1 and we can draw boundaries that separate them and classify new
documents. How exactly this is done is the topic of this chapter.

Whether or not a set of documents is mapped into a contiguous region de-
pends on the particular choices we make for the document representation:

type of weighting, stop list etc. To see that the document representation is

crucial, consider the two classes written by a group vs. written by a single per-
son. Frequent occurrence of the first person pronoun I is evidence for the

single-person class. But that information is likely deleted from the document
representation if we use a stop list. If the document representation chosen
is unfavorable, the contiguity hypothesis will not hold and successful vector
space classification is not possible.
The same considerations that led us to prefer weighted representations, in
particular length-normalized tf-idf representations, in Chapters 6 and 7 also

Online edition (c)

2009 Cambridge UP

290 14 Vector space classification

x
x
x
x
⋄
⋄

⋄
⋄

⋄

⋄

China

Kenya

UK

⋆

◮ Figure 14.1 Vector space classification into three classes.

apply here. For example, a term with 5 occurrences in a document should get
a higher weight than a term with one occurrence, but a weight 5 times larger
would give too much emphasis to the term. Unweighted and unnormalized
counts should not be used in vector space classification.

We introduce two vector space classification methods in this chapter, Roc-
chio and kNN. Rocchio classification (Section 14.2) divides the vector space

PROTOTYPE into regions centered on centroids or prototypes, one for each class, computed
as the center of mass of all documents in the class. Rocchio classification is
simple and efficient, but inaccurate if classes are not approximately spheres
with similar radii.
kNN or k nearest neighbor classification (Section 14.3) assigns the majority
class of the k nearest neighbors to a test document. kNN requires no explicit
training and can use the unprocessed training set directly in classification.
It is less efficient than other classification methods in classifying documents.
If the training set is large, then kNN can handle non-spherical and other
complex classes better than Rocchio.

A large number of text classifiers can be viewed as linear classifiers – clas-
sifiers that classify based on a simple linear combination of the features (Sec-
tion 14.4). Such classifiers partition the space of features into regions sepa-
rated by linear decision hyperplanes, in a manner to be detailed below. Because

of the bias-variance tradeoff (Section 14.6) more complex nonlinear models

Online edition (c)

2009 Cambridge UP

14.1 Document representations and measures of relatedness in vector spaces 291

dtrue

dprojected
x1

x2 x3 x4

x5

x
′
1
x
′
2
x
′
3
x
′
4
x
′
5
x
′
1
x
′
2 x
′
3
x
′
4
x
′
5

◮ Figure 14.2 Projections of small areas of the unit sphere preserve distances. Left:
A projection of the 2D semicircle to 1D. For the points x1
, x2, x3, x4
, x5 at x coordinates
−0.9, −0.2, 0, 0.2, 0.9 the distance |x2x3| ≈ 0.201 only differs by 0.5% from |x
′
2
x
′
3
| =

0.2; but |x1x3|/|x
′
1
x
′
3
| = dtrue/dprojected ≈ 1.06/0.9 ≈ 1.18 is an example of a large
distortion (18%) when projecting a large area. Right: The corresponding projection of
the 3D hemisphere to 2D.

are not systematically better than linear models. Nonlinear models have
more parameters to fit on a limited amount of training data and are more
likely to make mistakes for small and noisy data sets.
When applying two-class classifiers to problems with more than two classes,
there are one-of tasks – a document must be assigned to exactly one of several
mutually exclusive classes – and any-of tasks – a document can be assigned to
any number of classes as we will explain in Section 14.5. Two-class classifiers
solve any-of problems and can be combined to solve one-of problems.

14.1 Document representations and measures of relatedness in vec-
tor spaces

As in Chapter 6, we represent documents as vectors in R|V|

in this chapter.
To illustrate properties of document vectors in vector classification, we will
render these vectors as points in a plane as in the example in Figure 14.1.
In reality, document vectors are length-normalized unit vectors that point
to the surface of a hypersphere. We can view the 2D planes in our figures
as projections onto a plane of the surface of a (hyper-)sphere as shown in
Figure 14.2. Distances on the surface of the sphere and on the projection
plane are approximately the same as long as we restrict ourselves to small
areas of the surface and choose an appropriate projection (Exercise 14.1).

Online edition (c)

2009 Cambridge UP

292 14 Vector space classification

Decisions of many vector space classifiers are based on a notion of dis-
tance, e.g., when computing the nearest neighbors in kNN classification.

We will use Euclidean distance in this chapter as the underlying distance
measure. We observed earlier (Exercise 6.18, page 131) that there is a direct

correspondence between cosine similarity and Euclidean distance for length-
normalized vectors. In vector space classification, it rarely matters whether

the relatedness of two documents is expressed in terms of similarity or dis-
tance.

However, in addition to documents, centroids or averages of vectors also

play an important role in vector space classification. Centroids are not length-
normalized. For unnormalized vectors, dot product, cosine similarity and

Euclidean distance all have different behavior in general (Exercise 14.6). We

will be mostly concerned with small local regions when computing the sim-
ilarity between a document and a centroid, and the smaller the region the

more similar the behavior of the three measures is.
?
Exercise 14.1
For small areas, distances on the surface of the hypersphere are approximated well
by distances on its projection (Figure 14.2) because α ≈ sin α for small angles. For
what size angle is the distortion α/ sin(α) (i) 1.01, (ii) 1.05 and (iii) 1.1?